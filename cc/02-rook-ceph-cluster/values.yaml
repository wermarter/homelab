rook-ceph-cluster:
  toolbox:
    enabled: true
  cephClusterSpec:
    mon:
      count: 1
    mgr:
      count: 1
    dashboard:
      ssl: false
    logCollector:
      enabled: false
    crashCollector:
      disable: true
    resources:
      mgr:
        requests:
          cpu: "10m"
          memory: "128Mi"
      mon:
        requests:
          cpu: "10m"
          memory: "128Mi"
      osd:
        requests:
          cpu: "10m"
          memory: "128Mi"
    storage:
      useAllNodes: true
      useAllDevices: true
  cephBlockPools:
    - name: ceph-blockpool
      spec:
        failureDomain: host
        replicated:
          size: 1
      storageClass:
        enabled: true
        name: ceph-block
        annotations: {}
        labels: {}
        isDefault: true
        reclaimPolicy: Delete
        allowVolumeExpansion: true
        volumeBindingMode: "Immediate"
        mountOptions: []
        allowedTopologies: []
        parameters:
          imageFormat: "2"
          imageFeatures: layering
          csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
          csi.storage.k8s.io/provisioner-secret-namespace: "{{ .Release.Namespace }}"
          csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
          csi.storage.k8s.io/controller-expand-secret-namespace: "{{ .Release.Namespace }}"
          csi.storage.k8s.io/controller-publish-secret-name: rook-csi-rbd-provisioner
          csi.storage.k8s.io/controller-publish-secret-namespace: "{{ .Release.Namespace }}"
          csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
          csi.storage.k8s.io/node-stage-secret-namespace: "{{ .Release.Namespace }}"
          # Specify the filesystem type of the volume. If not specified, csi-provisioner
          # will set default as `ext4`. Note that `xfs` is not recommended due to potential deadlock
          # in hyperconverged settings where the volume is mounted on the same node as the osds.
          csi.storage.k8s.io/fstype: ext4
  cephFileSystems:
    - name: ceph-filesystem
      spec:
        metadataPool:
          replicated:
            size: 1
        dataPools:
          - failureDomain: host
            replicated:
              size: 1
            # Optional and highly recommended, 'data0' by default, see https://github.com/rook/rook/blob/master/Documentation/CRDs/Shared-Filesystem/ceph-filesystem-crd.md#pools
            name: data0
        metadataServer:
          activeCount: 1
          activeStandby: true
          resources:
            requests:
              cpu: "10m"
              memory: "256Mi"
          priorityClassName: system-cluster-critical
      storageClass:
        enabled: true
        isDefault: false
        name: ceph-filesystem
        # (Optional) specify a data pool to use, must be the name of one of the data pools above, 'data0' by default
        pool: data0
        reclaimPolicy: Delete
        allowVolumeExpansion: true
        volumeBindingMode: "Immediate"
        annotations: {}
        labels: {}
        mountOptions: []
        # see https://github.com/rook/rook/blob/master/Documentation/Storage-Configuration/Shared-Filesystem-CephFS/filesystem-storage.md#provision-storage for available configuration
        parameters:
          # The secrets contain Ceph admin credentials.
          csi.storage.k8s.io/provisioner-secret-name: rook-csi-cephfs-provisioner
          csi.storage.k8s.io/provisioner-secret-namespace: "{{ .Release.Namespace }}"
          csi.storage.k8s.io/controller-expand-secret-name: rook-csi-cephfs-provisioner
          csi.storage.k8s.io/controller-expand-secret-namespace: "{{ .Release.Namespace }}"
          csi.storage.k8s.io/controller-publish-secret-name: rook-csi-cephfs-provisioner
          csi.storage.k8s.io/controller-publish-secret-namespace: "{{ .Release.Namespace }}"
          csi.storage.k8s.io/node-stage-secret-name: rook-csi-cephfs-node
          csi.storage.k8s.io/node-stage-secret-namespace: "{{ .Release.Namespace }}"
          # Specify the filesystem type of the volume. If not specified, csi-provisioner
          # will set default as `ext4`. Note that `xfs` is not recommended due to potential deadlock
          # in hyperconverged settings where the volume is mounted on the same node as the osds.
          csi.storage.k8s.io/fstype: ext4
  cephObjectStores:
    - name: ceph-objectstore
      # see https://github.com/rook/rook/blob/master/Documentation/CRDs/Object-Storage/ceph-object-store-crd.md#object-store-settings for available configuration
      spec:
        metadataPool:
          failureDomain: host
          replicated:
            size: 1
        dataPool:
          failureDomain: host
          erasureCoded:
            dataChunks: 2
            codingChunks: 1
          parameters:
            bulk: "true"
        preservePoolsOnDelete: true
        gateway:
          port: 80
          resources:
            requests:
              cpu: "10m"
              memory: "128Mi"
          instances: 1
          priorityClassName: system-cluster-critical
      storageClass:
        enabled: true
        name: ceph-bucket
        reclaimPolicy: Delete
        volumeBindingMode: "Immediate"
        annotations: {}
        labels: {}
        # see https://github.com/rook/rook/blob/master/Documentation/Storage-Configuration/Object-Storage-RGW/ceph-object-bucket-claim.md#storageclass for available configuration
        parameters:
          # note: objectStoreNamespace and objectStoreName are configured by the chart
          region: us-east-1
      ingress:
        enabled: false
      route:
        enabled: false
